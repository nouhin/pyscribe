{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current configuration:\n",
      "Source folder: video_input\n",
      "Output folder: subs_out\n",
      "Model: medium\n",
      "Language: french\n",
      "Device: cuda\n",
      "Current configuration:\n",
      "Source folder: video_input\n",
      "Output folder: subs_out\n",
      "Model: tiny\n",
      "Language: french\n",
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import logging\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import ffmpeg\n",
    "import numpy as np\n",
    "import torch\n",
    "import whisper\n",
    "import whisper.utils\n",
    "import classes\n",
    "\n",
    "source_folder = Path('video_input')\n",
    "output_folder = Path('subs_out')\n",
    "model = 'tiny'\n",
    "language = 'french'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "#####################################################################\n",
    "\n",
    "print('Current configuration:')\n",
    "print(f'Source folder: {source_folder}')\n",
    "print(f'Output folder: {output_folder}')\n",
    "print(f'Model: {model}')\n",
    "print(f'Language: {language}')\n",
    "print(f'Device: {device}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whisper.available_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-01 15:11:38,700 - root - INFO - This is an info message.\n",
      "2023-10-01 15:11:38,701 - root - ERROR - \u001b[91mThis is an error message.\u001b[0m\n",
      "2023-10-01 15:11:38,703 - root - INFO - Loading model tiny on cuda\n",
      "2023-10-01 15:11:41,485 - root - INFO - Loaded model tiny on cuda\n"
     ]
    }
   ],
   "source": [
    "classes.setup_logging()\n",
    "\n",
    "logging.info(\"This is an info message.\")\n",
    "logging.error(\"This is an error message.\")\n",
    "\n",
    "# Init Whisperer\n",
    "whisperer = classes.Whisperer(output_folder=output_folder, model_name=model, language=language, device=device)\n",
    "whisperer.init_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-01 15:11:41,496 - root - INFO - Found 1 videos to process\n"
     ]
    }
   ],
   "source": [
    "# get list of videos in source folder, extension .mp4 mkv\n",
    "videos = [f for f in source_folder.iterdir() if f.is_file() and f.suffix in ['.mp4', '.mkv']]\n",
    "logging.info(f\"Found {len(videos)} videos to process\")\n",
    "\n",
    "# read context from file\n",
    "context_file = Path('context.txt')\n",
    "context = context_file.read_text() if context_file.exists() else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-01 15:11:41,509 - root - INFO - Processing video 2023-04-28 15-41-25_edit.mp4\n",
      "2023-10-01 15:11:41,510 - root - INFO - Generating subtitles for video_input\\2023-04-28 15-41-25_edit.mp4\n",
      "2023-10-01 15:11:41,512 - root - INFO - Loading audio from video_input\\2023-04-28 15-41-25_edit.mp4\n",
      "2023-10-01 15:11:41,747 - root - INFO - Loaded audio from video_input\\2023-04-28 15-41-25_edit.mp4\n",
      "c:\\Users\\winsid\\mambaforge\\envs\\pytrans\\Lib\\site-packages\\whisper\\timing.py:42: UserWarning: Failed to launch Triton kernels, likely due to missing CUDA toolkit; falling back to a slower median kernel implementation...\n",
      "  warnings.warn(\n",
      "c:\\Users\\winsid\\mambaforge\\envs\\pytrans\\Lib\\site-packages\\whisper\\timing.py:146: UserWarning: Failed to launch Triton kernels, likely due to missing CUDA toolkit; falling back to a slower DTW implementation...\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:00.740 --> 00:04.160]  Je comprends pas, il est pas plus certaine annuel, qu'est ce qui s'est passé.\n",
      "[00:08.040 --> 00:08.800]  On relâche l'intérêt.\n",
      "[00:10.500 --> 00:14.800]  Il est passant annuel, on va venir pour vous le repasser en auto.\n",
      "[00:17.200 --> 00:19.580]  Et il faut qu'on est étudié, qu'est ce qui s'est passé avant le repasser en auto.\n",
      "[00:20.100 --> 00:21.240]  On peut forcément le rentrer en auto.\n",
      "[00:23.480 --> 00:25.080]  Et tu couches de l'un bonan, je vous relâche.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\winsid\\mambaforge\\envs\\pytrans\\Lib\\site-packages\\whisper\\timing.py:42: UserWarning: Failed to launch Triton kernels, likely due to missing CUDA toolkit; falling back to a slower median kernel implementation...\n",
      "  warnings.warn(\n",
      "c:\\Users\\winsid\\mambaforge\\envs\\pytrans\\Lib\\site-packages\\whisper\\timing.py:146: UserWarning: Failed to launch Triton kernels, likely due to missing CUDA toolkit; falling back to a slower DTW implementation...\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:25.080 --> 00:37.760]  Je vois justement d'humanement c'estでは.\n",
      "[00:41.700 --> 00:44.620]  Je tenais très.\n",
      "[00:45.760 --> 00:46.180]  Je l'mise.\n",
      "[00:46.560 --> 00:51.840]  J'l'l'en avais main suis tout le monde.\n",
      "[00:51.840 --> 00:53.980]  J'en ai mal pour勇 voler beaucoup de SUN Paulo.\n",
      "[00:53.980 --> 00:55.000]  Je fuge le\n",
      "[00:55.000 --> 01:00.420]  compte que quand on fera un autre point après pour les modifs collectionnera de monde\n",
      "[01:00.420 --> 01:02.060]  et sur le monde.\n",
      "[01:02.160 --> 01:05.500]  Ou ce qu'on a discuté ensemble, ça marche.\n",
      "[01:06.820 --> 01:08.080]  Merci à vous tous.\n",
      "[01:08.500 --> 01:10.320]  Bon oui, c'est bon à tous.\n",
      "[01:16.740 --> 01:19.580]  Le coup, la va être surroulage, des effectivement, des passants magnuels, quoi.\n",
      "[01:19.920 --> 01:22.300]  Et j'aimerais, et on reviendra comprendre pourquoi.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-01 15:11:57,547 - root - INFO - Done transcription in 15.8 sec\n",
      "2023-10-01 15:11:57,548 - root - INFO - Saving raw output for video_input\\2023-04-28 15-41-25_edit.mp4\n",
      "2023-10-01 15:11:57,562 - root - INFO - Saved raw output for video_input\\2023-04-28 15-41-25_edit.mp4\n",
      "2023-10-01 15:11:57,563 - root - INFO - Saving subtitles for video_input\\2023-04-28 15-41-25_edit.mp4\n",
      "2023-10-01 15:11:57,564 - root - INFO - Saving subtitles for video_input\\2023-04-28 15-41-25_edit.mp4 to 2023-04-28 15-41-25_edit.srt\n",
      "2023-10-01 15:11:57,571 - root - INFO - Generated subtitles for video_input\\2023-04-28 15-41-25_edit.mp4\n",
      "2023-10-01 15:11:57,572 - root - INFO - Processed video 2023-04-28 15-41-25_edit.mp4 in 16.1 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:29.720 --> 01:32.240]  Donc on n'a pas de traiter d'incident pour avoir fonds ensemble sur ça.\n",
      "[01:34.140 --> 01:37.480]  Mais il y a tout qu'il y a été présenté pour que vous pîiez comme les débeguer.\n",
      "[01:39.060 --> 01:40.220]  Et pendant ce type d'incident.\n"
     ]
    }
   ],
   "source": [
    "# Process videos\n",
    "for video in videos:\n",
    "    tic = time.time()\n",
    "    logging.info(f\"Processing video {video.name}\")\n",
    "    video_obj = classes.Video(video, save_raw=True)\n",
    "    video_obj.context = context\n",
    "    video_obj.generate_subtitles(whisperer)\n",
    "    logging.info(f\"Processed video {video.name} in {time.time() - tic:.1f} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_obj.raw_output_path / f'{video_obj.path.stem}.mp4'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytrans",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
